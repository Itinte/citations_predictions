{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import networkx as nx\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quentinbb/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODES INFO INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODES INFO INDEX\n",
    "#1: YEAR\n",
    "#2: TITLE\n",
    "#3: AUTHORS\n",
    "#4: DOMAIN\n",
    "#5: ABSTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.getcwd()+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the files path\n",
    "train_file = 'training_set.txt'\n",
    "test_file = 'testing_set.txt'\n",
    "nodes_info_file = 'node_information.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the stopwords file\n",
    "def loadStopwords(path):\n",
    "    # import stopwords file\n",
    "    stopwords_file = open(path, 'r')\n",
    "    stopwords = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords.append(word.strip('\\n'))\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to clean sentences (lower case, stopwords, punctuation...) and authors\n",
    "def clean_sentence(sentence, stopwords, auth=False):\n",
    "    rx = re.compile('\\W+')\n",
    "    if auth is False:\n",
    "        sentence = str(sentence).lower().split()\n",
    "    else: \n",
    "        sentence = str(sentence).lower().split(',')\n",
    "    sentence = [i for i in sentence if i not in stopwords and len(str(i))>2]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts to convert probability predictions to bool predictions\n",
    "def raw_to_label(raw_preds):\n",
    "    text_label =[]\n",
    "    for i in range(len(raw_preds)):\n",
    "        num_label = np.argmax(raw_preds[i])\n",
    "        text_label.append(int(num_label))\n",
    "    \n",
    "    return np.asarray(text_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a domain to a dummy variable\n",
    "def map_field():\n",
    "    #initialize the domain dictionary\n",
    "    nodes = pd.read_csv('node_information.csv', header = None, index_col = 0)\n",
    "    fields = set(nodes[4])\n",
    "    i = 0\n",
    "    field_dic = dict()\n",
    "    for f in fields:\n",
    "        field_dic[f] = i\n",
    "        i+=1\n",
    "    return field_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec class to proceed to NLP analysis\n",
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.word2id = {}\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "  \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                self.word2id[word] = i\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "        \n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        words = np.array(self.word2vec.keys())\n",
    "        if w in words:\n",
    "            compute_score = lambda x: self.score(w, x)\n",
    "            vscore = np.vectorize(compute_score)\n",
    "            top_scores = np.argsort(vscore(words))[-K:]\n",
    "            return [words[i] for i in top_scores][::-1]\n",
    "        else:\n",
    "            return 'KeyError - Target word is out of vocabulary'\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        try: \n",
    "            return np.dot(self.word2vec[w1], self.word2vec[w2]) / (np.linalg.norm(self.word2vec[w1])*np.linalg.norm(self.word2vec[w2]))\n",
    "        except KeyError:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of vectors class to process the nlp analysis of the sentences\n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        sentemb = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent_mean = np.mean([self.w2v.word2vec[w] if w in self.w2v.word2vec else np.ones(300,)*0.001 for w in sent.split()], axis=0)\n",
    "            assert sent_mean.shape == (300,)\n",
    "            sentemb.append(sent_mean)\n",
    "\n",
    "        #print('Encoding completed')\n",
    "        return np.vstack(sentemb)  \n",
    "\n",
    "    def get_simil(self, s, sentences): #get the percentile of the similarity between the two sentences in the samples set\n",
    "        \n",
    "        # get most similar sentences and **print** them\n",
    "        sentemb = self.encode(sentences)\n",
    "        idx = sentences.values.tolist().index(s)\n",
    "        #idx0 = sentences.index[sentences == s]\n",
    "        keys = sentemb.copy()\n",
    "        keys = keys / np.linalg.norm(keys, 2, 1)[:, None]  # normalize embeddings\n",
    "        scores = keys[idx].dot(keys.T)  # dot-product of normalized vector = cosine similarity\n",
    "\n",
    "        #idxs = scores.argsort()[::-1][1:K+1]\n",
    "        \n",
    "        return scores#[sentences[idx] for idx in idxs]\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        sentences = []\n",
    "        sentences.append(s1)\n",
    "        sentences.append(s2)\n",
    "        sentemb = self.encode(sentences)\n",
    "        \n",
    "        src_index = sentences.index(s1)\n",
    "        trg_index = sentences.index(s2)\n",
    "        \n",
    " \n",
    "        keys = sentemb.copy()\n",
    "        src_key = keys[src_index] / np.linalg.norm(keys[src_index])\n",
    "        trg_key = keys[trg_index] / np.linalg.norm(keys[trg_index])\n",
    " \n",
    "        if not isinstance(src_key.dot(trg_key.T), float):\n",
    "            return 0\n",
    "        else:\n",
    "            return src_key.dot(trg_key.T)  # dot-product of normalized vector = cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "#instantiate the base of the NLP analysis\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=300000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "nodes = pd.read_csv('node_information.csv', header = None, index_col = 0)\n",
    "titles = nodes[2]\n",
    "abstracts = nodes[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â GRAPH ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize graph\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read nodes file\n",
    "train_data = np.genfromtxt(train_file, delimiter = ' ', dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 27684\n",
      "Number of edges: 334690\n"
     ]
    }
   ],
   "source": [
    "#create the graph based on the edge list\n",
    "for i in range(train_data.shape[0]):\n",
    "    if train_data[i][2] == 1:\n",
    "        G.add_edge(train_data[i][0], train_data[i][1])\n",
    "print('Number of nodes:', len(G.nodes()))\n",
    "print('Number of edges:', len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the original file: 27770\n"
     ]
    }
   ],
   "source": [
    "#check that the number of edges is quite close from the number of edges in the recently created graph\n",
    "nodes_features = pd.read_csv(nodes_info_file, header = None, index_col = 0)\n",
    "print('Number of nodes in the original file:', len(nodes_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delta of 27770 - 27684 nodes which means that only 16 nodes are not connected but present in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the Graph degree dictionnary\n",
    "degree_dict = dict(G.degree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal distance feature between the two papers\n",
    "def tmp_dist(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    return source_features[1] - target_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOL feature if source is older than target\n",
    "def older_than(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    return (source_features[1] < target_features[1])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBR of common words in the title after removing stopwords\n",
    "def common_w_title(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_w = set(clean_sentence(source_features[2], stopwords))\n",
    "    trg_w = set(clean_sentence(target_features[2], stopwords))\n",
    "    return len(list(set(src_w).intersection(trg_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBR of common words in the abstract after removing stopwords\n",
    "def common_w(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_w = set(clean_sentence(source_features[5], stopwords))\n",
    "    trg_w = set(clean_sentence(target_features[5], stopwords))\n",
    "    return len(list(set(src_w).intersection(trg_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBR common author (CAN BE IMPROVED, 'univeristy' is caught as an author)\n",
    "def common_auth(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_auth = set(clean_sentence(source_features[3], stopwords, auth=True))\n",
    "    trg_auth = set(clean_sentence(target_features[3], stopwords, auth =True))\n",
    "    common_auth = list(set(src_auth).intersection(trg_auth))\n",
    "    if common_auth == ['nan']:\n",
    "        common_auth = []\n",
    "    return len(common_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to check if the two papers are part of a common domain, ex: physics\n",
    "def common_dom(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_dom = source_features[4] \n",
    "    trg_dom = target_features[4]\n",
    "    \n",
    "    return (trg_dom == src_dom)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec similarity of the two titles\n",
    "def title_score(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_title = source_features[2]\n",
    "    trg_title = target_features[2]\n",
    "    \n",
    "    return np.exp(np.exp(s2v.score(src_title, trg_title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec similarity of the two abstracts\n",
    "def ab_score(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    source_features = LP.nodes_features.loc[source]\n",
    "    target_features = LP.nodes_features.loc[target]\n",
    "    src_ab = source_features[5]\n",
    "    trg_ab = target_features[5]\n",
    "    \n",
    "    return np.exp(np.exp(s2v.score(src_ab, trg_ab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First node degree centrality\n",
    "def Adegree(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    try:\n",
    "        return degree_dict[source]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second node degree centrality\n",
    "def Bdegree(tuple_i):\n",
    "    target = tuple_i[1]\n",
    "    try:\n",
    "        return degree_dict[target]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard similarity of nodes\n",
    "def jac_sim(tuple_i):\n",
    "    source = tuple_i[0]\n",
    "    target = tuple_i[1]\n",
    "    if (source in G.nodes) and (target in G.nodes):\n",
    "        preds = nx.jaccard_coefficient(G, [tuple_i])\n",
    "        for u, v, p in preds:\n",
    "            return p\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing class to simplify the computation of a training/testing dataset\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, part):\n",
    "        self.part= part #fraction of the training on which we want to train the model\n",
    "\n",
    "    def train_preprocess(self, train_file, nodes_info_file, features_list):\n",
    "        print('Building the train edges list...')\n",
    "        edges_train, y_train = self.get_train_edges(train_file)\n",
    "        to_keep = random.sample(range(len(edges_train)), k=int((len(edges_train)*self.part/100)))\n",
    "        edges_train_sub = [edges_train[i] for i in to_keep]\n",
    "        y_train_sub0 = [y_train[i] for i in to_keep]\n",
    "        y_train_sub = np_utils.to_categorical(y_train_sub0, num_classes=2)\n",
    "        print('Training on', self.part, '% of the the available data...')\n",
    "        print('Training size:', len(edges_train_sub))\n",
    "        print('Building x_train...')\n",
    "        x_train_sub0 = self.build_x_list(edges_train_sub, nodes_info_file, features_list)\n",
    "        x_train_sub = np.asarray(x_train_sub0)\n",
    "        print('Preprocessing finished...')\n",
    "        \n",
    "        return x_train_sub, y_train_sub\n",
    "    \n",
    "    def test_preprocess(self, test_file):\n",
    "        print('Building the test edges list...')\n",
    "        edges_test = self.get_test_edges(test_file)\n",
    "        edges_test_sub = edges_test[:]\n",
    "        print('Building x_test...')\n",
    "        x_test_sub0 = self.build_x_list(edges_test_sub, nodes_info_file, features_list)\n",
    "        x_test_sub = np.asarray(x_test_sub0)\n",
    "        #x_test_sub = normalize(x_test_sub)\n",
    "        return x_test_sub\n",
    "\n",
    "    def build_x_list(self, tuples_list, nodes_info_file, features_list): #method for building the features of the tuples \n",
    "        self.nodes_features = self.load_nodes_info(nodes_info_file)\n",
    "        features = []\n",
    "        for i in range(len(tuples_list)):#main loop over the edges list to find the features of each tuple\n",
    " \n",
    "            features.append(self.compute_tuple_features(tuples_list[i], features_list))       \n",
    "            \n",
    "        return features\n",
    "    \n",
    "\n",
    "    def compute_tuple_features(self, tuple_i, features_list): #method to get the features of a single tuples based on a list a features to compute\n",
    "        return self.map_funcs(tuple_i, features_list)\n",
    "     \n",
    "        \n",
    "    def map_funcs(self, obj, func_list): #map list of functions to an object and return a list comprehensions of the results\n",
    "        return [func(obj) for func in func_list] \n",
    "        \n",
    "    \n",
    "    def load_nodes_info(self, node_file): #get load the features indexed by node (use .loc to get the features of a node)\n",
    "        nodes_features = pd.read_csv(node_file, header = None, index_col = 0)\n",
    "        \n",
    "        return nodes_features\n",
    "\n",
    "    \n",
    "    def get_train_edges(self, train_file): #build the links_train and y_train\n",
    "        train_data = np.genfromtxt(train_file, delimiter = ' ', dtype = int)\n",
    "        edges_arr = train_data[:,:-1]\n",
    "        edges_tup = [tuple(row) for row in edges_arr] #list of tuple (edges) to be predicted for training\n",
    "        y_train = train_data[:,-1]\n",
    "\n",
    "        return edges_tup, y_train\n",
    "    \n",
    "\n",
    "    def get_test_edges(self, test_file): #build the links_test\n",
    "        test_data = np.genfromtxt(test_file, delimiter = ' ', dtype = int)\n",
    "        edges_tup = [tuple(row) for row in test_data] #list of tuple (edges) to be predicted for testing\n",
    "\n",
    "        return edges_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /!\\Â Long computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stopwords\n",
    "stopwords = loadStopwords('stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 features chosen !\n"
     ]
    }
   ],
   "source": [
    "#define the features you want\n",
    "features_list = [older_than, tmp_dist, title_score, ab_score, common_w, common_w_title, common_auth, common_dom, Adegree, Bdegree, jac_sim]\n",
    "print(len(features_list), 'features chosen !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "P = Preprocessor(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training...\n",
      "Building the train edges list...\n",
      "Training on 100 % of the the available data...\n",
      "Training size: 615512\n",
      "Building x_train...\n",
      "Preprocessing finished...\n",
      "Building the test edges list...\n",
      "Building x_test...\n",
      "Saving training files\n",
      "Preprocessing testing...\n",
      "Building the test edges list...\n",
      "Building x_test...\n",
      "Saving testing\n"
     ]
    }
   ],
   "source": [
    "#compute the training file\n",
    "print('Preprocessing training...')\n",
    "x_train, y_train = P.train_preprocess(train_file, nodes_info_file, features_list)\n",
    "x_test = P.test_preprocess(test_file)\n",
    "#save the training files to avoid recomputing all the training\n",
    "print('Saving training files')\n",
    "np.savetxt('train.csv', x_train, delimiter=',') \n",
    "np.savetxt('y_train.csv', y_train, delimiter=',')\n",
    "#preprocess test file\n",
    "print('Preprocessing testing...')\n",
    "x_test = P.test_preprocess(test_file)\n",
    "#save preprocessing to csv\n",
    "print('Saving testing')\n",
    "np.savetxt('test.csv', x_test, delimiter=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD PREPROCESSED FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Loading the preprocessed files to avoid recomputing all the training/testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get saved training file name\n",
    "x_train_f = 'train.csv'\n",
    "y_train_f = 'y_train.csv'\n",
    "test_f = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved files\n",
    "x_train = np.loadtxt(x_train_f, delimiter=',')\n",
    "y_train = np.loadtxt(y_train_f, delimiter=',')\n",
    "test = np.loadtxt(test_f, delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (615512, 11)\n",
      "y _train shape: (615512, 2)\n",
      "test shape: (32648, 11)\n"
     ]
    }
   ],
   "source": [
    "#check loaded files size\n",
    "print('x train shape:', x_train.shape)\n",
    "print('y _train shape:', y_train.shape)\n",
    "print('test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 553960 samples, validate on 61552 samples\n",
      "Epoch 1/5\n",
      "553960/553960 [==============================] - 24s 44us/step - loss: 0.1626 - acc: 0.9403 - val_loss: 0.1192 - val_acc: 0.9596\n",
      "Epoch 2/5\n",
      "553960/553960 [==============================] - 23s 41us/step - loss: 0.1308 - acc: 0.9548 - val_loss: 0.1153 - val_acc: 0.9614\n",
      "Epoch 3/5\n",
      "553960/553960 [==============================] - 25s 44us/step - loss: 0.1257 - acc: 0.9574 - val_loss: 0.1106 - val_acc: 0.9643\n",
      "Epoch 4/5\n",
      "553960/553960 [==============================] - 24s 43us/step - loss: 0.1230 - acc: 0.9588 - val_loss: 0.1176 - val_acc: 0.9632\n",
      "Epoch 5/5\n",
      "553960/553960 [==============================] - 23s 42us/step - loss: 0.1242 - acc: 0.9594 - val_loss: 0.1445 - val_acc: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa8dfdb4a8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "nhid = 16\n",
    "model.add(Dense(nhid, input_dim = len(features_list), activation = 'relu'))\n",
    "model.add(Dense(nhid, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "optim = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split = 0.1, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â LOGISTIC REGRESSSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform one hot encoded y_train to y_train\n",
    "y_train_h = [np.argmax(y_train[i]) for i in range(len(y_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9573477040252668\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 1)\n",
    "lr.fit(x_train, y_train_h)\n",
    "print(lr.score(x_train, y_train_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features importance: [9.73412712e-02 9.36310089e-02 5.59695782e-03 4.85462452e-03\n",
      " 4.56978611e-02 3.12778102e-02 7.28961927e-03 3.81447973e-04\n",
      " 4.00711411e-02 1.22992030e-01 5.50866228e-01]\n",
      "Training set score: 0.9696577808393663\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=13, random_state=0, max_features = 'auto', n_estimators = 20) #15\n",
    "rf.fit(x_train, y_train_h)\n",
    "print('Features importance:', rf.feature_importances_)\n",
    "print('Training set score:', rf.score(x_train, y_train_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96614245, 0.96820574])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estimate final leaderboard score with the cv validation score\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.1, random_state=0)\n",
    "print('Cross Validation score')\n",
    "cross_val_score(rf, x_train, y_train_h, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [11, 8, 6], 'max_depth': [8, 10, 12, 14], 'n_estimators': [10, 15, 20, 25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run grid search to find the best parameters of the random forest\n",
    "parameters = {'max_depth':[8,10,12,14], 'n_estimators':[10, 15, 20, 25], 'max_features':[11,8,6]}\n",
    "rf = RandomForestClassifier()\n",
    "gd_rf = GridSearchCV(rf, parameters)\n",
    "gd_rf.fit(x_train, y_train_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print best model found via gridsearch\n",
    "print('Best parameters found:', gd_rf.best_params_)\n",
    "print('Cross Validation score:', gd_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train best grid search model\n",
    "gd_rf.best_estimator_.fit(x_train, y_train_h)\n",
    "print('Features importance:', gd_rf.best_estimator_.feature_importances_)\n",
    "print('Training set score:', gd_rf.best_estimator_.score(x_train, y_train_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â TEST PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute prediction based on selected features\n",
    "y_predicts = rf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check predictions \n",
    "y_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw predictions to label predictions\n",
    "y_preds = raw_to_label(y_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5218390100465572"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check mean of predictions\n",
    "np.mean(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert prediction to dataframe in order to save them\n",
    "df = pd.DataFrame(y_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save DF to csv\n",
    "df.to_csv('predictions.csv', index = True, header = ['category'], index_label = 'id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
